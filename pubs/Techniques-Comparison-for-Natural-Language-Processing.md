<link rel="stylesheet" href="citation-copy.css">
<script src="citation-copy.js"></script>

<img src="/icons/unlock.svg" width="16" height="16"> Conference Paper

# Techniques Comparison for Natural Language Processing

Olena Iosifova <a href="https://orcid.org/0000-0001-6507-0761" target="_blank"><img src="/icons/orcid.svg" width="16" height="16"></a>,
Ievgen Iosifov <a href="https://orcid.org/0000-0001-6203-9945" target="_blank"><img src="/icons/orcid.svg" width="16" height="16"></a>,
Oleksandr Rolik <a href="https://orcid.org/0000-0001-8829-4645" target="_blank"><img src="/icons/orcid.svg" width="16" height="16"></a>,
<a href="/">Volodymyr Sokolov</a> <a href="https://orcid.org/0000-0002-9349-7946" target="_blank"><img src="/icons/orcid.svg" width="16" height="16"></a>

## Abstract

These improvements open many possibilities in solving Natural Language Processing downstream tasks. Such tasks include machine translation, speech recognition, information retrieval, sentiment analysis, summarization, question answering, multilingual dialogue systems development, and many more. Language models are one of the most important components in solving each of the mentioned tasks. This paper is devoted to research and analysis of the most adopted techniques and designs for building and training language models that show a state of the art results. Techniques and components applied in the creation of language models and its parts are observed in this paper, paying attention to neural networks, embedding mechanisms, bidirectionality, encoder and decoder architecture, attention, and self-attention, as well as parallelization through using transformer. As a result, the most promising techniques imply pre-training and fine-tuning of a language model, attention-based neural network as a part of model design, and a complex ensemble of multidimensional embedding to build deep context understanding. The latest offered architectures based on these approaches require a lot of computational power for training language models, and it is a direction of further improvement. Algorithm for choosing right model for relevant business task provided considering current challenges and available architectures.

## Links

* Full text: [PDF](http://ceur-ws.org/Vol-2631/paper5.pdf)

## Keywords

Attention; Decoder; Deep Learning; Embedding; Encoder; Gated Recurrent Unit; GRU; Language Model; Long Short-Term Memory; LSTM; Natural Language Processing; Neural Network; NLP; Recurrent Neural Network; RNN; Transfer Learning; Transformer

## Publisher

<table>
<tr>
<td>
<a href="https://www.scimagojr.com/journalsearch.php?q=21100218356&amp;tip=sid&amp;exact=no" title="SCImago Journal &amp; Country Rank"><img border="0" src="https://www.scimagojr.com/journal_img.php?id=21100218356" alt="SCImago Journal &amp; Country Rank"  /></a>
</td>
</tr>
</table>

[2020 2nd International Workshop on Modern Machine Learning Technologies and Data Science (MoMLeT+DS)](https://ceur-ws.org/Vol-2386/)

2–3 June 2020 <img src="/icons/location-pin.svg" width="16" height="16"> Lviv-Shatsk, Ukraine

First Online: 3 July 2020

## Indices

* ISSN: [1613-0073](https://portal.issn.org/resource/ISSN/1613-0073) <img src="/icons/online.svg" width="16" height="16">
* EID: [2-s2.0-85088881294](http://www.scopus.com/record/display.url?origin=inward&eid=2-s2.0-85088881294)
* WOS: [000664104800005](https://www.webofscience.com/wos/woscc/full-record/WOS:000664104800005)
* URN: [urn:nbn:de:0074-2631-6](https://nbn-resolving.org/xml/urn:nbn:de:0074-2631-6)
* DBLP: [conf/momlet/IosifovaIRS20](https://dblp.org/rec/conf/momlet/IosifovaIRS20)
* KUBG: [31628](http://elibrary.kubg.edu.ua/id/eprint/31628/)

## Cite

### APA

<small class="citation">`Iosifova, O., Iosifov, I., Rolik, O., & Sokolov, V. (2020). Techniques Comparison for Natural Language Processing. In 2nd International Workshop on Modern Machine Learning Tech-nologies and Data Science (Vol. 2631, no. I, pp. 57–67).`</small>

### IEEE

<small class="citation">`O. Iosifova, I. Iosifov, O. Rolik, and V. Sokolov, “Techniques Comparison for Natural Language Processing,” 2nd International Workshop on Modern Machine Learning Tech-nologies and Data Science, vol. 2631, no. I, pp. 57–67, 2020.`</small>

### CEUR-WS

<small class="citation">`O. Iosifova, et al., Techniques Comparison for Natural Language Processing, in: 2nd International Workshop on Modern Machine Learning Technologies and Data Science, vol. 2631, no. I (2020) 57–67.`</small>
